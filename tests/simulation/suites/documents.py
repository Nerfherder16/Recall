"""
Document Suite — Tests document ingestion, cascade operations, and sibling retrieval.

Scenarios:
A. Ingest: upload plaintext and markdown, verify document + children created
B. Cascade pin/unpin: pin document cascades to all children
C. Cascade delete: deleting document removes all children
D. Sibling retrieval boost: searching for one chunk surfaces siblings
E. Duplicate rejection: same content cannot be ingested twice
"""

import asyncio
import time
import uuid

from tests.simulation.report import SuiteReport

from .base import BaseSuite


class DocumentSuite(BaseSuite):
    name = "documents"

    async def run(self) -> SuiteReport:
        t0 = time.monotonic()
        passed = True

        try:
            # ── Scenario A: Document Ingestion ──
            self.observe("=== Scenario A: Document Ingestion ===")

            uid = uuid.uuid4().hex[:8]

            # Ingest a plaintext document (long enough to produce multiple chunks)
            # ~4500 chars across 2 large paragraphs to force multiple chunks (MAX_CHUNK_CHARS=3000)
            text_content = (
                f"The Recall API server runs on port 8200 using FastAPI with async endpoints. "
                f"Qdrant provides vector search at port 6333 with the recall_memories collection "
                f"storing 1024-dimensional embeddings generated by qwen3-embedding:0.6b. "
                f"Neo4j handles graph storage on bolt://localhost:7687 for relationship traversal "
                f"using spreading activation with configurable decay rates per hop. "
                f"Redis caches sessions on port 6379 with keys like recall:session:id and "
                f"recall:signals:pending:id for the async signal queue. "
                f"PostgreSQL stores audit logs, session archives, and metrics snapshots "
                f"using raw asyncpg connections with fire-and-forget writes. "
                f"The retrieval pipeline has multiple stages: vector search, sub-embedding boost, "
                f"graph expansion with spreading activation, anti-pattern checking, "
                f"temporal recency weighting, inhibition for contradictions, and final scoring. "
                f"Each stage can be independently configured through the search API. "
                f"The memory model includes fields for content, memory_type, domain, tags, "
                f"importance, stability, access_count, pinned status, and durability tier. "
                f"Durability tiers are permanent (immune to decay), durable (0.15x decay rate), "
                f"and ephemeral (normal decay). Pinned memories are also immune to decay. "
                f"The consolidation worker merges similar memories using LLM to produce "
                f"concise summaries with DERIVED_FROM relationships in Neo4j. "
                f"Pattern extraction runs at 3:30am identifying recurring themes across memories. "
                f"The feedback system tracks useful and not-useful memories to adjust importance "
                f"and co-retrieval strengthening between frequently co-used memory pairs. "
                f"Anti-patterns are stored in a separate Qdrant collection and checked during "
                f"retrieval to inject warnings when problematic patterns are detected. "
                f"The system processes over 1000 memories with sub-second retrieval latency. "
                f"Rate limiting is applied via slowapi with configurable per-endpoint limits. "
                f"Document {uid} infrastructure section covers the complete tech stack.\n\n"
                f"Ollama runs LLM inference on a separate machine at 192.168.50.62:11434 "
                f"with an RTX 3090 GPU providing 24GB VRAM for model execution. "
                f"The embedding model qwen3-embedding:0.6b produces 1024-dimensional vectors "
                f"and is used for both memory storage and search query embedding. "
                f"The primary LLM is qwen3:14b configured with think:false for JSON output "
                f"to prevent the model from including reasoning tokens in structured responses. "
                f"All LLM calls use a 180 second timeout with asyncio.Semaphore(1) to "
                f"prevent resource saturation on the shared GPU. The Ollama API supports "
                f"streaming but Recall uses synchronous JSON mode for structured extraction. "
                f"The deployment runs on CasaOS at 192.168.50.19 using Docker containers "
                f"orchestrated by docker compose with volume mounts for hot-reload capability. "
                f"Volumes are mounted so code changes only need a container restart, not a "
                f"full image rebuild which saves significant development iteration time. "
                f"The docker-compose.yml defines services for api, worker, qdrant, neo4j, "
                f"redis, and postgres with health checks verifying all services are operational. "
                f"SCP is used to deploy individual changed files then docker compose restart "
                f"picks up the modifications. The signal detection system uses the LLM to "
                f"classify conversation turns into memory types including facts, decisions, "
                f"preferences, patterns, workflows, and error fixes with durability classification. "
                f"Each detected signal has a confidence score and importance rating from 1-10. "
                f"High-confidence signals are auto-stored while medium-confidence signals go "
                f"to a pending queue for manual review by the user through the dashboard. "
                f"The worker processes signals asynchronously using ARQ with Redis as the "
                f"job queue backend providing reliable at-least-once delivery semantics. "
                f"The dashboard is built with React, Vite, Tailwind CSS, and DaisyUI "
                f"providing real-time updates via Server-Sent Events from the API. "
                f"Multi-user support allows per-user API keys with shared memory visibility. "
                f"Document {uid} deployment section covers the complete operational setup."
            )

            text_result = await self.client.ingest_document(
                content=text_content.encode(),
                filename=f"infra-{uid}.txt",
                domain=self.domain,
                file_type="text",
            )

            if text_result and text_result.get("document", {}).get("id"):
                doc = text_result["document"]
                text_doc_id = doc["id"]
                text_child_ids = text_result.get("child_ids", [])
                self.observe(f"Text ingested: {doc['filename']}, "
                             f"{text_result['memories_created']} memories, "
                             f"type={doc['file_type']}")

                if text_result["memories_created"] < 1:
                    self.error("Text ingestion produced 0 memories")
                    passed = False
            else:
                self.error(f"Text ingestion failed: {text_result}")
                text_doc_id = None
                text_child_ids = []
                passed = False

            await asyncio.sleep(2)

            # Ingest a markdown document
            md_content = (
                f"# Deployment Guide {uid}\n\n"
                f"The application is deployed on CasaOS at 192.168.50.19.\n\n"
                f"## Docker Setup\n\n"
                f"All services run in Docker containers managed by docker compose. "
                f"The API image is volume-mounted so restarts pick up code changes.\n\n"
                f"## Monitoring\n\n"
                f"Prometheus metrics are exposed at /metrics. "
                f"The dashboard provides real-time system health views.\n"
            )

            md_result = await self.client.ingest_document(
                content=md_content.encode(),
                filename=f"deploy-{uid}.md",
                domain=self.domain,
                file_type="markdown",
            )

            if md_result and md_result.get("document", {}).get("id"):
                md_doc = md_result["document"]
                md_doc_id = md_doc["id"]
                md_child_ids = md_result.get("child_ids", [])
                self.observe(f"Markdown ingested: {md_doc['filename']}, "
                             f"{md_result['memories_created']} memories")
            else:
                self.error(f"Markdown ingestion failed: {md_result}")
                md_doc_id = None
                md_child_ids = []
                passed = False

            await asyncio.sleep(2)

            self.metric("ingestion", {
                "text_memories": len(text_child_ids),
                "markdown_memories": len(md_child_ids),
                "text_doc_id": text_doc_id,
                "markdown_doc_id": md_doc_id,
            })

            # ── Scenario B: List and Detail ──
            self.observe("\n=== Scenario B: List and Detail ===")

            docs = await self.client.list_documents(domain=self.domain)
            self.observe(f"Listed {len(docs)} documents in domain {self.domain}")

            if len(docs) < 2:
                self.observe(f"Warning: expected >=2 documents, got {len(docs)}")

            # Get detail with child IDs
            if text_doc_id:
                detail = await self.client.get_document(text_doc_id)
                if detail and "child_memory_ids" in detail:
                    self.observe(f"Detail: {len(detail['child_memory_ids'])} child IDs")
                else:
                    self.error("Document detail missing child_memory_ids")

            self.metric("listing", {
                "documents_in_domain": len(docs),
            })

            await asyncio.sleep(1)

            # ── Scenario C: Cascade Pin/Unpin ──
            self.observe("\n=== Scenario C: Cascade Pin/Unpin ===")

            if text_doc_id and text_child_ids:
                # Pin the document
                pin_result = await self.client.pin_document(text_doc_id)
                if pin_result and pin_result.get("pinned"):
                    self.observe(f"Document pinned, children_pinned={pin_result.get('children_pinned')}")
                else:
                    self.error(f"Pin document failed: {pin_result}")

                await asyncio.sleep(1)

                # Verify a child is pinned
                child_mem = await self.client.get_memory(text_child_ids[0])
                child_pinned = child_mem.get("pinned") if child_mem else None
                self.observe(f"Child pinned status: {child_pinned}")

                if child_pinned is not True:
                    self.error("Cascade pin did not propagate to child")
                    passed = False

                # Unpin
                unpin_result = await self.client.unpin_document(text_doc_id)
                if unpin_result and unpin_result.get("pinned") is False:
                    self.observe("Document unpinned")
                else:
                    self.error(f"Unpin document failed: {unpin_result}")

                await asyncio.sleep(1)

                # Verify child unpinned
                child_mem2 = await self.client.get_memory(text_child_ids[0])
                child_pinned2 = child_mem2.get("pinned") if child_mem2 else None
                self.observe(f"Child pinned after unpin: {child_pinned2}")

                self.metric("cascade_pin", {
                    "pin_propagated": child_pinned is True,
                    "unpin_propagated": child_pinned2 is not True,
                })
            else:
                self.observe("Skipping pin cascade — no text document")

            await asyncio.sleep(2)

            # ── Scenario D: Sibling Retrieval Boost ──
            self.observe("\n=== Scenario D: Sibling Retrieval Boost ===")

            if text_child_ids and len(text_child_ids) >= 2:
                # Search for content from the first chunk
                results = await self.client.search_query(
                    "FastAPI port 8200 vector search Qdrant",
                    limit=10,
                    domains=[self.domain],
                    expand_relationships=True,
                )

                result_ids = [r.get("id") for r in results]
                siblings_found = sum(1 for cid in text_child_ids if cid in result_ids)
                self.observe(f"Search returned {len(results)} results, "
                             f"{siblings_found}/{len(text_child_ids)} siblings found")

                self.metric("sibling_boost", {
                    "total_results": len(results),
                    "total_siblings": len(text_child_ids),
                    "siblings_in_results": siblings_found,
                    "sibling_coverage": round(siblings_found / len(text_child_ids), 2)
                        if text_child_ids else 0,
                })
            else:
                self.observe("Skipping sibling test — not enough children")

            await asyncio.sleep(2)

            # ── Scenario E: Duplicate Rejection ──
            self.observe("\n=== Scenario E: Duplicate Rejection ===")

            dup_uid = uuid.uuid4().hex[:8]
            dup_content = f"Deterministic duplicate test content {dup_uid}"

            dup1 = await self.client.ingest_document(
                content=dup_content.encode(),
                filename=f"dup1-{dup_uid}.txt",
                domain=self.domain,
            )

            if dup1 and dup1.get("document", {}).get("id"):
                self.observe(f"First upload succeeded: {dup1['document']['id'][:8]}...")

                # Try same content again
                dup2_result = await self._raw_ingest(
                    content=dup_content.encode(),
                    filename=f"dup2-{dup_uid}.txt",
                    domain=self.domain,
                )

                if dup2_result and dup2_result.get("status") == 409:
                    self.observe("Duplicate correctly rejected (409)")
                elif dup2_result and dup2_result.get("status") and dup2_result["status"] >= 400:
                    self.observe(f"Duplicate rejected with status {dup2_result['status']}")
                else:
                    self.observe("Warning: duplicate was not rejected")

                self.metric("dedup", {
                    "first_accepted": True,
                    "duplicate_rejected": (dup2_result or {}).get("status") == 409,
                })
            else:
                self.error("First upload failed for dedup test")

            await asyncio.sleep(1)

            # ── Scenario F: Cascade Delete ──
            self.observe("\n=== Scenario F: Cascade Delete ===")

            if md_doc_id and md_child_ids:
                del_result = await self.client.delete_document(md_doc_id)
                if del_result and del_result.get("deleted"):
                    self.observe(f"Document deleted, children_deleted={del_result.get('children_deleted')}")

                    # Verify children are gone
                    await asyncio.sleep(1)
                    gone_count = 0
                    for cid in md_child_ids[:3]:
                        mem = await self.client.get_memory(cid)
                        if mem is None:
                            gone_count += 1
                    self.observe(f"Verified {gone_count}/{min(len(md_child_ids), 3)} children deleted")

                    self.metric("cascade_delete", {
                        "children_deleted": del_result.get("children_deleted", 0),
                        "verified_gone": gone_count,
                    })

                    # Remove from tracked to avoid double-cleanup
                    if md_doc_id in self.client.tracked_documents:
                        self.client.tracked_documents.remove(md_doc_id)
                    for cid in md_child_ids:
                        if cid in self.client.tracked_ids:
                            self.client.tracked_ids.remove(cid)
                else:
                    self.error(f"Cascade delete failed: {del_result}")
                    passed = False
            else:
                self.observe("Skipping cascade delete — no markdown document")

            # ── Scenario G: Durability Inheritance ──
            self.observe("\n=== Scenario G: Durability Inheritance ===")

            dur_uid = uuid.uuid4().hex[:8]
            dur_content = (
                f"Durability inheritance test {dur_uid}. "
                f"CasaOS runs at 192.168.50.19 with Docker containers."
            )

            dur_result = await self.client.ingest_document(
                content=dur_content.encode(),
                filename=f"perm-{dur_uid}.txt",
                domain=self.domain,
                durability="permanent",
            )

            if dur_result and dur_result.get("child_ids"):
                dur_doc = dur_result["document"]
                dur_children = dur_result["child_ids"]
                self.observe(f"Ingested with durability=permanent, "
                             f"{len(dur_children)} children")

                # Check document durability
                doc_dur = dur_doc.get("durability")
                self.observe(f"Document durability: {doc_dur}")

                # Check child durability
                if dur_children:
                    child = await self.client.get_memory(dur_children[0])
                    child_dur = child.get("durability") if child else None
                    self.observe(f"Child durability: {child_dur}")

                    self.metric("durability_inheritance", {
                        "document_durability": doc_dur,
                        "child_durability": child_dur,
                        "inherited": child_dur == "permanent",
                    })

                    if child_dur != "permanent":
                        self.error(f"Child didn't inherit permanent durability: {child_dur}")
                        passed = False
            else:
                self.error(f"Durability document ingestion failed: {dur_result}")

        except Exception as e:
            self.error(f"Document suite exception: {e}")
            passed = False

        return self._make_report(passed, time.monotonic() - t0)

    async def _raw_ingest(self, content: bytes, filename: str,
                          domain: str) -> dict | None:
        """Raw ingest that returns status code for error checking."""
        import httpx
        try:
            async with httpx.AsyncClient(timeout=120.0) as upload_client:
                r = await upload_client.post(
                    f"{self.client.base}/document/ingest",
                    files={"file": (filename, content, "text/plain")},
                    data={"domain": domain, "file_type": "text"},
                    headers={"Authorization": f"Bearer {self.client.api_key}"},
                )
            return {"status": r.status_code, "body": r.json() if r.text else {}}
        except Exception:
            return None
